{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim\n",
    "import pickle\n",
    "import nltk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Affect Dimension == emotion angry 0 fear 1 joy 2 sadness 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### angry  ###\n",
    "file_angry = pd.read_csv(\"../data/anger_train.txt\",sep=\"\t\")\n",
    "file_angry = file_angry.drop(columns=\"ID\")\n",
    "f = open(\"./text_preprocessed_angry.txt\", 'w',encoding = 'utf-8')\n",
    "for i in range(0,len(file_angry[\"Affect Dimension\"])):\n",
    "    text = ''\n",
    "    file_angry[\"Affect Dimension\"][i] = 0\n",
    "    split_text = file_angry[\"Intensity Class\"][i].split(\":\")\n",
    "    file_angry[\"Intensity Class\"][i] = split_text[0]\n",
    "    temp = re.sub('[^a-zA-Z\\s@]', '', file_angry[\"Tweet\"][i])\n",
    "    temp = temp.split(\" \")\n",
    "    temp_list = []\n",
    "    for j in range(0,len(temp)):\n",
    "        if \"@\" in temp[j]:\n",
    "            temp_list.append(\"<user>\")\n",
    "            text = text +\"<user>\"+' '\n",
    "        elif \"\" ==temp[j]:\n",
    "            pass\n",
    "        else:\n",
    "            temp_list.append(temp[j].lower())\n",
    "            text = text + temp[j].lower()+' '\n",
    "    text=text+'\\n'\n",
    "    f.write(text)\n",
    "    file_angry[\"Tweet\"][i] = temp_list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fear  ###\n",
    "file_fear = pd.read_csv(\"../data/fear_train.txt\",sep=\"\t\")\n",
    "file_fear = file_fear.drop(columns=\"ID\")\n",
    "f = open(\"./text_preprocessed_fear.txt\", 'w',encoding = 'utf-8')\n",
    "for i in range(0,len(file_fear[\"Affect Dimension\"])):\n",
    "    text = ''\n",
    "    file_fear[\"Affect Dimension\"][i] = 1\n",
    "    split_text = file_fear[\"Intensity Class\"][i].split(\":\")\n",
    "    file_fear[\"Intensity Class\"][i] = split_text[0]\n",
    "    temp = re.sub('[^a-zA-Z\\s@#]', '', file_fear[\"Tweet\"][i])\n",
    "    temp = temp.split(\" \")\n",
    "    temp_list = []\n",
    "    for j in range(0,len(temp)):\n",
    "        if \"@\" in temp[j]:\n",
    "            temp_list.append(\"<user>\")\n",
    "            text = text +\"<user>\"+' '\n",
    "        elif \"#\" in temp[j]: #일단 #도 지움\n",
    "            temp_list.append(\"<hashtag>\")\n",
    "            text = text +\"<hashtag>\"+' '\n",
    "        elif \"\" ==temp[j]:\n",
    "            pass\n",
    "        else:\n",
    "            temp_list.append(temp[j].lower())\n",
    "            text = text + temp[j].lower()+' '\n",
    "    text=text+'\\n'\n",
    "    f.write(text)\n",
    "    file_fear[\"Tweet\"][i] = temp_list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### joy  ###\n",
    "file_joy = pd.read_csv(\"../data/joy_train.txt\",sep=\"\t\")\n",
    "file_joy = file_joy.drop(columns=\"ID\")\n",
    "f = open(\"./text_preprocessed_joy.txt\", 'w',encoding = 'utf-8')\n",
    "for i in range(0,len(file_joy[\"Affect Dimension\"])):\n",
    "    text = ''\n",
    "    file_joy[\"Affect Dimension\"][i] = 2\n",
    "    split_text = file_joy[\"Intensity Class\"][i].split(\":\")\n",
    "    file_joy[\"Intensity Class\"][i] = split_text[0]\n",
    "    temp = re.sub('[^a-zA-Z\\s@#]', '', file_joy[\"Tweet\"][i])\n",
    "    temp = temp.split(\" \")\n",
    "    temp_list = []\n",
    "    for j in range(0,len(temp)):\n",
    "        if \"@\" in temp[j]:\n",
    "            temp_list.append(\"<user>\")\n",
    "            text = text +\"<user>\"+' '\n",
    "        elif \"#\" in temp[j]: #일단 #도 지움\n",
    "            temp_list.append(\"<hashtag>\")\n",
    "            text = text +\"<hashtag>\"+' '\n",
    "        elif \"\" ==temp[j]:\n",
    "            pass\n",
    "        else:\n",
    "            temp_list.append(temp[j].lower())\n",
    "            text = text + temp[j].lower()+' '\n",
    "    text=text+'\\n'\n",
    "    f.write(text)\n",
    "    file_joy[\"Tweet\"][i] = temp_list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sadness  ###\n",
    "file_sadness = pd.read_csv(\"../data/sadness_train.txt\",sep=\"\t\")\n",
    "file_sadness = file_sadness.drop(columns=\"ID\")\n",
    "f = open(\"./text_preprocessed_sadness.txt\", 'w',encoding = 'utf-8')\n",
    "for i in range(0,len(file_sadness[\"Affect Dimension\"])):\n",
    "    text = ''\n",
    "    file_sadness[\"Affect Dimension\"][i] = 3\n",
    "    split_text = file_sadness[\"Intensity Class\"][i].split(\":\")\n",
    "    file_sadness[\"Intensity Class\"][i] = split_text[0]\n",
    "    temp = re.sub('[^a-zA-Z\\s@#]', '', file_sadness[\"Tweet\"][i])\n",
    "    temp = temp.split(\" \")\n",
    "    temp_list = []\n",
    "    for j in range(0,len(temp)):\n",
    "        if \"@\" in temp[j]:\n",
    "            temp_list.append(\"<user>\")\n",
    "            text = text +\"<user>\"+' '\n",
    "        elif \"#\" in temp[j]: #일단 #도 지움\n",
    "            temp_list.append(\"<hashtag>\")\n",
    "            text = text +\"<hashtag>\"+' '\n",
    "        elif \"\" ==temp[j]:\n",
    "            pass\n",
    "        else:\n",
    "            temp_list.append(temp[j].lower())\n",
    "            text = text + temp[j].lower()+' '\n",
    "    text=text+'\\n'\n",
    "    f.write(text)\n",
    "    file_sadness[\"Tweet\"][i] = temp_list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file_angry\n",
    "# file = pd.concat([file_angry, file_fear],ignore_index=True)\n",
    "# file = pd.concat([file, file_joy],ignore_index=True)\n",
    "# file = pd.concat([file, file_sadness],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_affect = file.iloc[:,1:2]\n",
    "one_hot_intensity = file.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect = copy.deepcopy(file.iloc[:,1:2])\n",
    "intensity = copy.deepcopy(file.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(one_hot_affect)\n",
    "one_hot_affect = pd.DataFrame(enc.transform(one_hot_affect).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc2.fit(one_hot_intensity)\n",
    "one_hot_intensity = pd.DataFrame(enc2.transform(one_hot_intensity).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([file[\"Tweet\"],one_hot_intensity],axis = 1)\n",
    "#data = pd.concat([data,one_hot_intensity],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pos = list()\n",
    "for i in data[\"Tweet\"]:\n",
    "    tags_en = nltk.pos_tag(i)\n",
    "    sentences_pos.append(tags_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training word2vec ...\")\n",
    "model = gensim.models.Word2Vec(data[\"Tweet\"], size=30, window=5,min_count=0, workers=4, iter=10, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dic = {n: i for i, n in enumerate(model.wv.vocab)}\n",
    "dic_num = {i: n for i, n in enumerate(model.wv.vocab)}\n",
    "num_dic[\"<start>\"] = len(num_dic)\n",
    "num_dic[\"<end>\"] = len(num_dic)\n",
    "num_dic[\"<unknown>\"] = len(num_dic)\n",
    "dic_num[len(dic_num)] = \"<start>\"\n",
    "dic_num[len(dic_num)] = \"<end>\"\n",
    "dic_num[len(dic_num)] = \"<unknown>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "output_batch = []\n",
    "for word in data[\"Tweet\"]:\n",
    "    # 인코더 셀의 입력값. 입력단어의 글자들을 한글자씩 떼어 배열로 만든다.\n",
    "    input = [num_dic[n] for n in word]\n",
    "    input_batch.append(input)\n",
    "    # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n",
    "    output = [num_dic[\"<start>\"]]\n",
    "    for i in word:\n",
    "        output.append(num_dic[i])\n",
    "    output_batch.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec에 <start>, UNK 등 추가 후 numpy로 저장\n",
    "key = list(num_dic.keys())\n",
    "w2v = []\n",
    "for k in key:\n",
    "    if k == '<start>' or k == '<unknown>' or k == \"<end>\":\n",
    "        w2v.append(np.random.randn(30)*0.1)\n",
    "    else:\n",
    "        w2v.append(model.wv[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/preprocessed_data.csv\",sep = \",\",index = False)\n",
    "with open('sentence.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('sentence_pos.pickle', 'wb') as f:\n",
    "    pickle.dump(sentences_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('sentence_real_data_input.pickle', 'wb') as f:\n",
    "    pickle.dump(input_batch, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('sentence_real_data_add_start_end.pickle', 'wb') as f:\n",
    "    pickle.dump(output_batch, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('num_dic.pickle', 'wb') as f:\n",
    "    pickle.dump(num_dic, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('dic_num.pickle', 'wb') as f:\n",
    "    pickle.dump(dic_num, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('word_embedding_matrix.pickle', 'wb') as f:\n",
    "    pickle.dump(w2v, f, pickle.HIGHEST_PROTOCOL)\n",
    "#### 감정 세기 데이터에서 1이 원본데이터에서의 0에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./before_train_text.txt', 'w',encoding = 'utf-8')\n",
    "for idx,sentence in enumerate(data[\"Tweet\"]):\n",
    "    for j in sentence:\n",
    "        file.write(j+' ')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../model/save/positive_file.txt', 'w')\n",
    "for idx,sentence in enumerate(input_batch):\n",
    "    #file.write(str(affect[\"Affect Dimension\"][idx])+' ')\n",
    "    file.write(str(intensity[\"Intensity Class\"][idx])+' ')\n",
    "    for word in sentence:\n",
    "        word = str(word) + ' '\n",
    "        file.write(word)\n",
    "    if len(sentence)<40:\n",
    "        for _ in range(0,40-len(sentence)):\n",
    "            file.write('4421 ')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다 학습시켜도 되려나? 일단 뭐... 학습해보라해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[&lt;user&gt;, &lt;user&gt;, shut, up, hashtags, are, cool...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[it, makes, me, so, fucking, irate, jesus, nob...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[lol, adam, the, bull, with, his, fake, outrage]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[&lt;user&gt;, passed, away, early, this, morning, i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[&lt;user&gt;, lol, wow, i, was, gonna, say, really,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Affect Dimension  \\\n",
       "0  [<user>, <user>, shut, up, hashtags, are, cool...                0   \n",
       "1  [it, makes, me, so, fucking, irate, jesus, nob...                0   \n",
       "2   [lol, adam, the, bull, with, his, fake, outrage]                0   \n",
       "3  [<user>, passed, away, early, this, morning, i...                0   \n",
       "4  [<user>, lol, wow, i, was, gonna, say, really,...                0   \n",
       "\n",
       "  Intensity Class  \n",
       "0               2  \n",
       "1               3  \n",
       "2               1  \n",
       "3               0  \n",
       "4               1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_angry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
